# End-to-End ETL Pipeline in Microsoft Azure - (e2e-de-azure)

This is my first data engineering project and my first non-academic project.

I created an end-to-end pipeline from an on-prem SQL Server Database

I connected Azure Data Factory, Azure Databricks, and Synapse Analytics to this Git repository for audit records and to have a centralized location to run processes from.

The extract/transform/load pipeline accesses and runs the notebooks from "/databricks-notebooks"

The dataset used was the Microsoft lightweight AdventureWorksLT2022 set: https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms

My goal in this project was to learn more about the development stages in a data engineering pipeline, and to gain hands-on practice with the Azure cloud platform and its many tools.

In this project, I used:

- Azure Gen2 Data Lake Storage
- Azure Data Factory
- Azure Databricks
- On-prem SQL Server Database
- SQL Server Management Studio (SSMS)
- Azure Synapse Analytics
- Serverless Azure SQL Database
- Azure Entra ID (prev. known as Azure Active Directory)
- Azure Key Vault
- Power BI Desktop

These are the major steps I took in creating this end-to-end project:

- 

<iframe title="pbi-e2e-de" width="1140" height="541.25" src="https://app.powerbi.com/reportEmbed?reportId=1acf642e-4e67-4e62-bc9f-23ced8921180&autoAuth=true&ctid=61dc8969-454c-42ba-8bac-44f842fcbc7f" frameborder="0" allowFullScreen="true"></iframe>
